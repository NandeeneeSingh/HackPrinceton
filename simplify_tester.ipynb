{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON data into pandas dataframe\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the list of definitions into a dataframe\n",
    "definitions = data['definitions']\n",
    "dataframe = pd.DataFrame(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1962 entries, 0 to 1961\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   term        1962 non-null   object\n",
      " 1   definition  1962 non-null   object\n",
      " 2   imgURL      20 non-null     object\n",
      " 3   altText     20 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 61.4+ KB\n",
      "None\n",
      "                    term                       definition  \\\n",
      "count               1962                             1962   \n",
      "unique              1962                             1935   \n",
      "top     24-hour coverage  To get better after being sick.   \n",
      "freq                   1                                2   \n",
      "\n",
      "                                imgURL  \\\n",
      "count                               20   \n",
      "unique                              19   \n",
      "top     ./images/2308a_The_Trachea.jpg   \n",
      "freq                                 2   \n",
      "\n",
      "                                                  altText  \n",
      "count                                                  20  \n",
      "unique                                                 20  \n",
      "top     The tubes (trachea and bronchi) going from the...  \n",
      "freq                                                    1  \n",
      "                 term                                         definition  \\\n",
      "0    24-hour coverage  When many health plans or programs are combine...   \n",
      "1           abatement  Remove or cover a dangerous material, such as ...   \n",
      "2  abdomen, abdominal                          Your belly or tummy area.   \n",
      "3             ability                              To able to or can do.   \n",
      "4            ablation  To remove or destroy a body part or tissue. Th...   \n",
      "\n",
      "  imgURL altText  \n",
      "0   None    None  \n",
      "1   None    None  \n",
      "2   None    None  \n",
      "3   None    None  \n",
      "4   None    None  \n"
     ]
    }
   ],
   "source": [
    "# EDA: Check the structure and summary of the dataframe\n",
    "print(dataframe.info())\n",
    "print(dataframe.describe())\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(dataframe, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS = Dataset.from_pandas(train_df)\n",
    "valDS = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    term = \" \".join(examples['term']) if isinstance(examples['term'], list) else examples['term']\n",
    "    inputTexts = [term for term in examples['term']]\n",
    "    targetTexts = examples['definition']\n",
    "\n",
    "    modelInputs = tokenizer(inputTexts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(targetTexts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    modelInputs['labels'] = labels['input_ids']\n",
    "    return modelInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1765/1765 [00:00<00:00, 3373.62 examples/s]\n",
      "Map: 100%|██████████| 197/197 [00:00<00:00, 6298.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainDS = trainDS.map(tokenize_function, batched=True)\n",
    "valDS = valDS.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingArgs = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingArgs,\n",
    "    train_dataset=trainDS,\n",
    "    eval_dataset=valDS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1110 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  1%|          | 10/1110 [06:43<7:56:02, 25.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.4278, 'grad_norm': 103.59442901611328, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1110 [13:50<8:24:08, 27.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.8668, 'grad_norm': 52.654869079589844, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1110 [15:38<3:45:19, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.644, 'grad_norm': 65.02728271484375, 'learning_rate': 3e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 40/1110 [18:12<4:11:40, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.304, 'grad_norm': 55.18180465698242, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 50/1110 [19:51<2:32:36,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.0799, 'grad_norm': 45.56892776489258, 'learning_rate': 5e-06, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 60/1110 [31:45<14:29:14, 49.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.7848, 'grad_norm': 56.29859924316406, 'learning_rate': 6e-06, 'epoch': 0.54}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(term):\n",
    "    inputText = f\"define: {term}\"\n",
    "    inputID = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max=128)\n",
    "\n",
    "        predictedDef = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return predictedDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_to_define = \"coronary artery bypass graft\"\n",
    "predictedDef = generate_model(term_to_define)\n",
    "print(f\"Definition of '{term_to_define}': {predictedDef}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
